Section A
1.
J48 -C 0.25 -M 2
Time taken to build model: 0.16 seconds
Overall accuracy  78.3585 %
Confusion Matrix:
  a    b   <-- classified as
 3838    0 |    a = 0
 1060    0 |    b = 1

2.
weka.classifiers.functions.SMO -C 1.0 -L 0.001 -P 1.0E-12 -N 0 -V -1 -W 1 -K "weka.classifiers.functions.supportVector.PolyKernel -C 250007 -E 1.0"
Time taken to build model: 343.87 seconds
Overall accuracy    82.9318 %
Confusion Matrix:
    a    b   <-- classified as
 3460  378 |    a = 0
  458  602 |    b = 1

3.
Logistic -R 1.0E-8 -M -1
Time taken to build model: 95.44 seconds
Overall accuracy   74.8673 %
Confusion Matrix:
    a    b   <-- classified as
 2997  841 |    a = 0
  390  670 |    b = 1

Section B

1.The result of Weka is 78.35% compared to my result 78.36%. Pretty close results. I believe after postpruning the trees, accuracy is going to increase. Here are the reasons:
Situations occur in which two attributes individually seem to have nothing
to contribute but are powerful predictors when combined¡ªa sort of combinationlock
effect in which the correct combination of the two attribute values is very
informative but the attributes taken individually are not. postpruning takes advantage of this, which lead to the improvement of the decision tree.
2.I choose Logistic Regression as my classifier.  Logistics regression deals with situation where binary variable as dependent variables.It uses logistics 
function to turn discrete 0,1 into continuous values ranging from 0 to 1 as the dependent variable, and regress
independent variables on it. 
Strength:  estimate the probability of a binary response based on one or more predictor (or independent) variables (features)
Weakness: not actually classify, it is to estimate probability of a binary response.

3.For the three algorithm I used, SVM gives the best performance and takes longest time to run, While decision tree takes the shortest time and give ok result.
Logistics takes longer time and gives bad accuracies.